{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pathlib import Path\n",
    "import openvino_genai as ov_genai\n",
    "from llm_config import convert_and_compress_model\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined settings\n",
    "MODEL_ID = \"gemma-2-9b-it\"\n",
    "COMPRESSION_VARIANT = \"INT4\"\n",
    "DEVICE = \"CPU\"\n",
    "USE_PRECONVERTED = True\n",
    "\n",
    "MODEL_CONFIGURATION = {\n",
    "    \"model_id\": \"google/gemma-2-9b-it\",\n",
    "    \"remote_code\": False,\n",
    "    \"start_message\": (\n",
    "        \"Ah, you’ve summoned me, mighty one! I am your friendly, magical assistant, ready to grant you \"\n",
    "        \"three wishes… er, well, actually, more than three answers! I’ll give you helpful, respectful, \"\n",
    "        \"and honest answers, all wrapped up in a mystical cloud of wisdom. Fear not, for I shall keep my answers \"\n",
    "        \"safe and positive, avoiding any dark magic, negativity, or harm. Ask away, and I’ll share knowledge fit for royalty!\"\n",
    "    ),\n",
    "    \"history_template\": (\n",
    "        \"<start_of_turn>user{user}<end_of_turn><start_of_turn>model{assistant}<end_of_turn>\"\n",
    "    ),\n",
    "    \"current_message_template\": (\n",
    "        \"<start_of_turn>user{user}<end_of_turn><start_of_turn>model{assistant}\"\n",
    "    ),\n",
    "    \"rag_prompt_template\": (\n",
    "        \"You are a genie assistant, here to grant wishes in the form of knowledge! Use the following pieces of \"\n",
    "        \"retrieved context to answer the question. If you don’t know the answer, just say that even a genie can’t \"\n",
    "        \"know it all. Be concise and keep the magic alive, just like a good wish, keep it sweet and to the point. \"\n",
    "        \"<start_of_turn>user{input}<end_of_turn><start_of_turn>context{context} <end_of_turn><start_of_turn>model\"\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ INT4 gemma-2-9b-it model already converted and can be found in gemma\\INT4_compressed_weights\n"
     ]
    }
   ],
   "source": [
    "def load_model():\n",
    "    \"\"\"\n",
    "    Loads and prepares the model with the specified configuration and compression variant.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_dir = convert_and_compress_model(\n",
    "            MODEL_ID,\n",
    "            MODEL_CONFIGURATION,\n",
    "            COMPRESSION_VARIANT,\n",
    "            USE_PRECONVERTED,\n",
    "        )\n",
    "        pipe = ov_genai.LLMPipeline(str(model_dir), DEVICE)\n",
    "        return pipe\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model setup: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the model\n",
    "pipe = load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aladdin is granting your wish...\n",
      "\n",
      "Abracadabra – here comes your wish!\n",
      "\n",
      "\n",
      "Morocco is located in **North Africa**. \n",
      "\n",
      "It is bordered by:\n",
      "\n",
      "* **Algeria** to the east and south\n",
      "* **Western Sahara** to the south\n",
      "* **Spain** (Ceuta and Melilla) to the north \n",
      "\n",
      "\n",
      "Let me know if you have any other questions about Morocco!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example input interaction\n",
    "input_prompt = input(\"What do you wish for today? \").strip()\n",
    "\n",
    "if input_prompt:\n",
    "    print(\"Aladdin is granting your wish...\")\n",
    "    time.sleep(1)  # Simulate processing\n",
    "    try:\n",
    "        output = pipe.generate(input_prompt, max_new_tokens=200)\n",
    "        print(\"\\nAbracadabra – here comes your wish!\")\n",
    "        print(output)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Please enter a valid wish.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wish History:\n",
      "\n",
      "Wish 1: Where is morraco?\n",
      "Response: \n",
      "\n",
      "Morocco is located in **North Africa**. \n",
      "\n",
      "It is bordered by:\n",
      "\n",
      "* **Algeria** to the east and south\n",
      "* **Western Sahara** to the south\n",
      "* **Spain** (Ceuta and Melilla) to the north \n",
      "\n",
      "\n",
      "Let me know if you have any other questions about Morocco!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manage history of interactions\n",
    "history = []\n",
    "\n",
    "# Example of adding to history\n",
    "if input_prompt:\n",
    "    try:\n",
    "        output = pipe.generate(input_prompt, max_new_tokens=200)\n",
    "        history.append({\"input\": input_prompt, \"output\": output})\n",
    "    except Exception as e:\n",
    "        print(f\"Error while generating output: {e}\")\n",
    "\n",
    "# Display history\n",
    "if history:\n",
    "    print(\"\\nWish History:\")\n",
    "    for i, entry in enumerate(history):\n",
    "        print(f\"\\nWish {i + 1}: {entry['input']}\")\n",
    "        print(f\"Response: {entry['output']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
